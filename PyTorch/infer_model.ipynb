{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c72c67-24d8-42a7-9272-e25bf68bf7dc",
   "metadata": {},
   "source": [
    "# Predict new flight positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257cf1d2-a1f3-477b-b640-4fddf3780274",
   "metadata": {},
   "source": [
    "## 1. Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7124869c-33c5-4afb-be5e-8e5ec27c4bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FlightModel(\n",
       "  (embedding_layer): PositionEmbedding(\n",
       "    (position_emb): Embedding(2048, 6)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=6, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Context size of our custom model\n",
    "context_size = 2048\n",
    "\n",
    "class PositionEmbedding(torch.nn.Module):\n",
    "    \"\"\"Token and positioning embedding layer for a sequence.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Init variables and layers.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.position_emb = torch.nn.Embedding(num_embeddings=context_size, embedding_dim=6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass.\"\"\"\n",
    "        len_input = x.size()[1]\n",
    "        positions = torch.arange(start=0, end=len_input, step=1).to(device)\n",
    "        position_embedding = self.position_emb(positions)\n",
    "        return x + position_embedding\n",
    "\n",
    "def create_attention_mask(key_length, query_length, dtype):\n",
    "    \"\"\"\n",
    "    Create a Casual Mask for\n",
    "    the multi head attention layer.\n",
    "    \"\"\"\n",
    "    i = torch.arange(query_length)[:, None]\n",
    "    j = torch.arange(key_length)\n",
    "    mask = i >= j - key_length + query_length\n",
    "    mask = torch.logical_not(mask)\n",
    "    mask = mask.to(dtype)\n",
    "    return mask\n",
    "\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    \"\"\"Transformer Block Layer.\"\"\"\n",
    "    def __init__(self, num_heads, embed_dim, ff_dim, mask_function, dropout_rate=0.1):\n",
    "        \"\"\"Init variables and layers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.MultiheadAttention(\n",
    "          embed_dim=embed_dim,\n",
    "          num_heads=num_heads,\n",
    "          batch_first=True,\n",
    "        )\n",
    "        self.dropout_1 = torch.nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(\n",
    "          normalized_shape=embed_dim, eps=1e-6\n",
    "        )\n",
    "        self.ffn_1 = torch.nn.Linear(\n",
    "          in_features=embed_dim, out_features=ff_dim\n",
    "        )\n",
    "        self.ffn_2 = torch.nn.Linear(\n",
    "          in_features=ff_dim, out_features=embed_dim\n",
    "        )\n",
    "        self.dropout_2 = torch.nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm(\n",
    "          normalized_shape=embed_dim, eps=1e-6\n",
    "        )\n",
    "        self.mask_function = mask_function\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward Pass.\"\"\"\n",
    "        seq_len = inputs.size()[1]\n",
    "        mask = self.mask_function(seq_len, seq_len, torch.bool).to(device)\n",
    "        attention_output, _ = self.attn(\n",
    "        query=inputs, key=inputs, value=inputs, attn_mask=mask\n",
    "        )\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        out1 = self.layer_norm_1(inputs + attention_output)\n",
    "        ffn_1 = self.relu(self.ffn_1(out1))\n",
    "        ffn_2 = self.ffn_2(ffn_1)\n",
    "        ffn_output = self.dropout_2(ffn_2)\n",
    "        output = self.layer_norm_2(out1 + ffn_output)\n",
    "        return output\n",
    "\n",
    "class FlightModel(torch.nn.Module):\n",
    "  def __init__(self, feed_forward_dim, num_heads):\n",
    "    \"\"\"Init Function.\"\"\"\n",
    "    super().__init__()\n",
    "    self.embedding_layer = PositionEmbedding()\n",
    "    self.transformer_layers = []\n",
    "    for i in range(6):\n",
    "        transformer = TransformerBlock(\n",
    "          num_heads=num_heads,\n",
    "          embed_dim=6,\n",
    "          ff_dim=feed_forward_dim,\n",
    "          mask_function=create_attention_mask,\n",
    "        ).to(device)\n",
    "        self.transformer_layers.append(transformer)\n",
    "        \n",
    "    self.output_layer = torch.nn.Linear(6, 6)\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    \"\"\"Forward Pass.\"\"\"\n",
    "    # Position embedding\n",
    "    embedding = self.embedding_layer(input_tensor)\n",
    "    # Transformer layers\n",
    "    transformer_output = self.transformer_layers[0](embedding)\n",
    "    for i in range(1, len(self.transformer_layers)):\n",
    "        transformer_output = self.transformer_layers[i](transformer_output)\n",
    "    # FC network\n",
    "    output = self.output_layer(transformer_output)\n",
    "    return output\n",
    "\n",
    "\n",
    "model = FlightModel(24, 3).to(device)\n",
    "model.load_state_dict(torch.load(\"./flight_prediction_model.pt\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efbe1e-d5f5-437d-9fe0-00c5c1074ab2",
   "metadata": {},
   "source": [
    "## 2. Generate new plane positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "facf31c7-bee8-49e1-9ccd-b90309875e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[   -33.6259,    -70.9092,  14550.0000,    327.9000,     22.0000,\n",
      "             -64.0000],\n",
      "         [   -33.6076,    -70.9004,  14225.0000,    324.4000,     21.9000,\n",
      "            -900.0000],\n",
      "         [    42.9495,    -56.7895,  39223.0078,    293.6164,    113.2114,\n",
      "           -1436.0114],\n",
      "         [    46.3945,    -70.1680,  38821.1602,    296.2093,    131.6245,\n",
      "           -1574.3026],\n",
      "         [    35.7285,    -52.3658,  28551.8594,    260.2770,     93.1539,\n",
      "           -1847.1328],\n",
      "         [    41.0644,    -46.8390,  35955.1445,    297.1762,    141.6188,\n",
      "           -2255.0793],\n",
      "         [    49.8091,    -84.7665,  37767.4062,    289.0022,    150.9227,\n",
      "           -1473.2308],\n",
      "         [    43.2783,    -57.0705,  37121.4766,    288.2540,    145.1998,\n",
      "           -1784.8833],\n",
      "         [    49.3376,    -85.3608,  38816.5938,    283.4930,    137.0054,\n",
      "           -1094.7213],\n",
      "         [    45.8777,    -83.6688,  35812.0625,    272.9915,    111.7598,\n",
      "           -1109.2737],\n",
      "         [    46.1801,    -65.4029,  39749.2930,    299.8903,    137.7093,\n",
      "           -1634.9047],\n",
      "         [    39.3167,    -25.4292,  31833.8594,    285.2741,    213.0979,\n",
      "           -2929.4175]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "plane_pos_1 = [-33.625946, -70.909181, 14550, 327.9, 22, -64]\n",
    "plane_pos_2 = [-33.607635, -70.900381, 14225, 324.4, 21.9, -900]\n",
    "plane_pos = [plane_pos_1, plane_pos_2]\n",
    "pos_input = torch.tensor(plane_pos)\n",
    "pos_input = pos_input.view(1, -1, 6)\n",
    "pos_input = pos_input.to(device)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(10):  \n",
    "        output = model(pos_input)\n",
    "        output = torch.squeeze(output)\n",
    "        output = output[-1]\n",
    "        div_tensor = torch.tensor([90,  180, 10000, 1000, 360, 1000]).to(device)\n",
    "        output = torch.mul(output, div_tensor)\n",
    "        output = output.view(1, 1, 6)\n",
    "        pos_input = torch.cat((pos_input, output), 1)\n",
    "\n",
    "print(pos_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
