{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c72c67-24d8-42a7-9272-e25bf68bf7dc",
   "metadata": {},
   "source": [
    "# Predict new flight positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257cf1d2-a1f3-477b-b640-4fddf3780274",
   "metadata": {},
   "source": [
    "## 1. Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7124869c-33c5-4afb-be5e-8e5ec27c4bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FlightModel(\n",
       "  (embedding_layer): PositionEmbedding(\n",
       "    (position_emb): Embedding(2048, 6)\n",
       "  )\n",
       "  (transformer1): TransformerBlock(\n",
       "    (attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm_1): LayerNorm((6,), eps=1e-06, elementwise_affine=True)\n",
       "    (ffn_1): Linear(in_features=6, out_features=20, bias=True)\n",
       "    (ffn_2): Linear(in_features=20, out_features=6, bias=True)\n",
       "    (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm_2): LayerNorm((6,), eps=1e-06, elementwise_affine=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (transformer2): TransformerBlock(\n",
       "    (attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm_1): LayerNorm((6,), eps=1e-06, elementwise_affine=True)\n",
       "    (ffn_1): Linear(in_features=6, out_features=20, bias=True)\n",
       "    (ffn_2): Linear(in_features=20, out_features=6, bias=True)\n",
       "    (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm_2): LayerNorm((6,), eps=1e-06, elementwise_affine=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (transformer3): TransformerBlock(\n",
       "    (attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm_1): LayerNorm((6,), eps=1e-06, elementwise_affine=True)\n",
       "    (ffn_1): Linear(in_features=6, out_features=20, bias=True)\n",
       "    (ffn_2): Linear(in_features=20, out_features=6, bias=True)\n",
       "    (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm_2): LayerNorm((6,), eps=1e-06, elementwise_affine=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (output_layer): Linear(in_features=6, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Context size of our custom model\n",
    "context_size = 2048\n",
    "\n",
    "class PositionEmbedding(torch.nn.Module):\n",
    "    \"\"\"Token and positioning embedding layer for a sequence.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Init variables and layers.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.position_emb = torch.nn.Embedding(num_embeddings=context_size, embedding_dim=6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass.\"\"\"\n",
    "        len_input = x.size()[1]\n",
    "        positions = torch.arange(start=0, end=len_input, step=1).to(device)\n",
    "        position_embedding = self.position_emb(positions)\n",
    "        return x + position_embedding\n",
    "\n",
    "def create_attention_mask(key_length, query_length, dtype):\n",
    "    \"\"\"\n",
    "    Create a Casual Mask for\n",
    "    the multi head attention layer.\n",
    "    \"\"\"\n",
    "    i = torch.arange(query_length)[:, None]\n",
    "    j = torch.arange(key_length)\n",
    "    mask = i >= j - key_length + query_length\n",
    "    mask = torch.logical_not(mask)\n",
    "    mask = mask.to(dtype)\n",
    "    return mask\n",
    "\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    \"\"\"Transformer Block Layer.\"\"\"\n",
    "    def __init__(self, num_heads, embed_dim, ff_dim, mask_function, dropout_rate=0.1):\n",
    "        \"\"\"Init variables and layers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.MultiheadAttention(\n",
    "          embed_dim=embed_dim,\n",
    "          num_heads=num_heads,\n",
    "          batch_first=True,\n",
    "        )\n",
    "        self.dropout_1 = torch.nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(\n",
    "          normalized_shape=embed_dim, eps=1e-6\n",
    "        )\n",
    "        self.ffn_1 = torch.nn.Linear(\n",
    "          in_features=embed_dim, out_features=ff_dim\n",
    "        )\n",
    "        self.ffn_2 = torch.nn.Linear(\n",
    "          in_features=ff_dim, out_features=embed_dim\n",
    "        )\n",
    "        self.dropout_2 = torch.nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm(\n",
    "          normalized_shape=embed_dim, eps=1e-6\n",
    "        )\n",
    "        self.mask_function = mask_function\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward Pass.\"\"\"\n",
    "        seq_len = inputs.size()[1]\n",
    "        mask = self.mask_function(seq_len, seq_len, torch.bool).to(device)\n",
    "        attention_output, _ = self.attn(\n",
    "        query=inputs, key=inputs, value=inputs, attn_mask=mask\n",
    "        )\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        out1 = self.layer_norm_1(inputs + attention_output)\n",
    "        ffn_1 = self.relu(self.ffn_1(out1))\n",
    "        ffn_2 = self.ffn_2(ffn_1)\n",
    "        ffn_output = self.dropout_2(ffn_2)\n",
    "        output = self.layer_norm_2(out1 + ffn_output)\n",
    "        return output\n",
    "\n",
    "class FlightModel(torch.nn.Module):\n",
    "  def __init__(self, feed_forward_dim, num_heads):\n",
    "    \"\"\"Init Function.\"\"\"\n",
    "    super().__init__()\n",
    "    self.embedding_layer = PositionEmbedding()\n",
    "    self.transformer_layers = []\n",
    "    for i in range(24):\n",
    "        transformer = TransformerBlock(\n",
    "          num_heads=num_heads,\n",
    "          embed_dim=6,\n",
    "          ff_dim=feed_forward_dim,\n",
    "          mask_function=create_attention_mask,\n",
    "        ).to(device)\n",
    "        self.transformer_layers.append(transformer)\n",
    "        \n",
    "    self.output_layer = torch.nn.Linear(6, 6)\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    \"\"\"Forward Pass.\"\"\"\n",
    "    # Position embedding\n",
    "    embedding = self.embedding_layer(input_tensor)\n",
    "    # Transformer layers\n",
    "    transformer_output = self.transformer_layers[0](embedding)\n",
    "    for i in range(1, len(self.transformer_layers)):\n",
    "        transformer_output = self.transformer_layers[i](transformer_output)\n",
    "    # FC network\n",
    "    output = self.output_layer(transformer_output)\n",
    "    return output\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    \"\"\"Forward Pass.\"\"\"\n",
    "    embedding = self.embedding_layer(input_tensor)\n",
    "    transformer_output = self.transformer1(embedding)\n",
    "    transformer_output = self.transformer2(transformer_output)\n",
    "    transformer_output = self.transformer3(transformer_output)\n",
    "    output = self.output_layer(transformer_output)\n",
    "    return output\n",
    "\n",
    "\n",
    "model = FlightModel(20, 2).to(device)\n",
    "model.load_state_dict(torch.load(\"./flight_prediction_model.pt\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efbe1e-d5f5-437d-9fe0-00c5c1074ab2",
   "metadata": {},
   "source": [
    "## 2. Generate new plane positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facf31c7-bee8-49e1-9ccd-b90309875e37",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \n\u001b[1;32m---> 10\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m         output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(output)\n\u001b[0;32m     12\u001b[0m         output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 117\u001b[0m, in \u001b[0;36mFlightModel.forward\u001b[1;34m(self, input_tensor)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_tensor):\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Forward Pass.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m   embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m   transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer1(embedding)\n\u001b[0;32m    119\u001b[0m   transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer2(transformer_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 34\u001b[0m, in \u001b[0;36mPositionEmbedding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m positions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end\u001b[38;5;241m=\u001b[39mlen_input, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     33\u001b[0m position_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_emb(positions)\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embedding\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "plane_pos_1 = [-33.625946, -70.909181, 14550, 327.9, 22, -64]\n",
    "plane_pos_2 = [-33.607635, -70.900381, 14225, 324.4, 21.9, -900]\n",
    "plane_pos = [plane_pos_1, plane_pos_2]\n",
    "pos_input = torch.tensor(plane_pos)\n",
    "pos_input = pos_input.view(1, -1, 6)\n",
    "pos_input = pos_input.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(10):  \n",
    "        output = model(pos_input)\n",
    "        output = torch.squeeze(output)\n",
    "        output = output[-1]\n",
    "        output = output.view(1, 1, 6)\n",
    "        pos_input = torch.cat((pos_input, output), 1)\n",
    "\n",
    "print(pos_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
