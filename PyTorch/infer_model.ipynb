{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c72c67-24d8-42a7-9272-e25bf68bf7dc",
   "metadata": {},
   "source": [
    "# Predict new flight positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257cf1d2-a1f3-477b-b640-4fddf3780274",
   "metadata": {},
   "source": [
    "## 1. Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7124869c-33c5-4afb-be5e-8e5ec27c4bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FlightModel(\n",
       "  (embedding_layer): PositionEmbedding(\n",
       "    (position_emb): Embedding(2048, 6)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=6, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Context size of our custom model\n",
    "context_size = 2048\n",
    "\n",
    "class PositionEmbedding(torch.nn.Module):\n",
    "    \"\"\"Token and positioning embedding layer for a sequence.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Init variables and layers.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.position_emb = torch.nn.Embedding(num_embeddings=context_size, embedding_dim=6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass.\"\"\"\n",
    "        len_input = x.size()[1]\n",
    "        positions = torch.arange(start=0, end=len_input, step=1).to(device)\n",
    "        position_embedding = self.position_emb(positions)\n",
    "        return x + position_embedding\n",
    "\n",
    "def create_attention_mask(key_length, query_length, dtype):\n",
    "    \"\"\"\n",
    "    Create a Casual Mask for\n",
    "    the multi head attention layer.\n",
    "    \"\"\"\n",
    "    i = torch.arange(query_length)[:, None]\n",
    "    j = torch.arange(key_length)\n",
    "    mask = i >= j - key_length + query_length\n",
    "    mask = torch.logical_not(mask)\n",
    "    mask = mask.to(dtype)\n",
    "    return mask\n",
    "\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    \"\"\"Transformer Block Layer.\"\"\"\n",
    "    def __init__(self, num_heads, embed_dim, ff_dim, mask_function, dropout_rate=0.1):\n",
    "        \"\"\"Init variables and layers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.MultiheadAttention(\n",
    "          embed_dim=embed_dim,\n",
    "          num_heads=num_heads,\n",
    "          batch_first=True,\n",
    "        )\n",
    "        self.dropout_1 = torch.nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(\n",
    "          normalized_shape=embed_dim, eps=1e-6\n",
    "        )\n",
    "        self.ffn_1 = torch.nn.Linear(\n",
    "          in_features=embed_dim, out_features=ff_dim\n",
    "        )\n",
    "        self.ffn_2 = torch.nn.Linear(\n",
    "          in_features=ff_dim, out_features=embed_dim\n",
    "        )\n",
    "        self.dropout_2 = torch.nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm(\n",
    "          normalized_shape=embed_dim, eps=1e-6\n",
    "        )\n",
    "        self.mask_function = mask_function\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward Pass.\"\"\"\n",
    "        seq_len = inputs.size()[1]\n",
    "        mask = self.mask_function(seq_len, seq_len, torch.bool).to(device)\n",
    "        attention_output, _ = self.attn(\n",
    "        query=inputs, key=inputs, value=inputs, attn_mask=mask\n",
    "        )\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        out1 = self.layer_norm_1(inputs + attention_output)\n",
    "        ffn_1 = self.ffn_1(out1)\n",
    "        ffn_2 = self.ffn_2(ffn_1)\n",
    "        ffn_output = self.dropout_2(ffn_2)\n",
    "        output = self.layer_norm_2(out1 + ffn_output)\n",
    "        return output\n",
    "\n",
    "class FlightModel(torch.nn.Module):\n",
    "  def __init__(self, feed_forward_dim, num_heads):\n",
    "    \"\"\"Init Function.\"\"\"\n",
    "    super().__init__()\n",
    "    self.embedding_layer = PositionEmbedding()\n",
    "    self.transformer_layers = []\n",
    "    for i in range(6):\n",
    "        transformer = TransformerBlock(\n",
    "          num_heads=num_heads,\n",
    "          embed_dim=6,\n",
    "          ff_dim=feed_forward_dim,\n",
    "          mask_function=create_attention_mask,\n",
    "        ).to(device)\n",
    "        self.transformer_layers.append(transformer)\n",
    "        \n",
    "    self.output_layer = torch.nn.Linear(6, 6)\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    \"\"\"Forward Pass.\"\"\"\n",
    "    # Position embedding\n",
    "    embedding = self.embedding_layer(input_tensor)\n",
    "    # Transformer layers\n",
    "    transformer_output = self.transformer_layers[0](embedding)\n",
    "    for i in range(1, len(self.transformer_layers)):\n",
    "        transformer_output = self.transformer_layers[i](transformer_output)\n",
    "    # FC network\n",
    "    output = self.output_layer(transformer_output)\n",
    "    return output\n",
    "\n",
    "\n",
    "model = FlightModel(24, 1).to(device)\n",
    "model.load_state_dict(torch.load(\"./flight_prediction_model_exp.pt\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efbe1e-d5f5-437d-9fe0-00c5c1074ab2",
   "metadata": {},
   "source": [
    "## 2. Generate new plane positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "facf31c7-bee8-49e1-9ccd-b90309875e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[   -33.6259,    -70.9092,  14550.0000,    327.9000,     22.0000,\n",
      "             -64.0000],\n",
      "         [   -33.6076,    -70.9004,  14225.0000,    324.4000,     21.9000,\n",
      "            -900.0000],\n",
      "         [    36.2456,    -26.7269,   1513.5985,     70.9948,    163.8928,\n",
      "            -101.4011],\n",
      "         [    47.6427,    -74.3880,   2403.3401,     41.0355,    201.9553,\n",
      "             -69.3190],\n",
      "         [    34.0646,    -43.2149,   1161.7010,    102.0613,    159.6414,\n",
      "             -42.8755],\n",
      "         [    30.8297,    -12.7706,    977.0361,     92.5054,    148.9762,\n",
      "             -93.7314],\n",
      "         [    30.6272,      2.6675,    904.8125,     72.3756,    109.5661,\n",
      "            -127.9924],\n",
      "         [    33.3599,      9.7938,   1246.7458,     44.9890,    121.7768,\n",
      "            -169.1501],\n",
      "         [    32.1715,     -3.9269,   1164.5562,     72.0250,    140.9974,\n",
      "            -127.6193],\n",
      "         [    36.9437,    -40.6239,   1469.4241,     82.9010,    178.1634,\n",
      "             -68.4417],\n",
      "         [    56.3929,    -89.2875,   2753.7795,      0.8116,    204.1822,\n",
      "             -71.4021],\n",
      "         [    49.0060,    -50.1440,   2421.6804,     13.2125,    192.1898,\n",
      "            -121.2259]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "plane_pos_1 = [-33.625946, -70.909181, 14550, 327.9, 22, -64]\n",
    "plane_pos_2 = [-33.607635, -70.900381, 14225, 324.4, 21.9, -900]\n",
    "plane_pos = [plane_pos_1, plane_pos_2]\n",
    "pos_input = torch.tensor(plane_pos)\n",
    "pos_input = pos_input.view(1, -1, 6)\n",
    "pos_input = pos_input.to(device)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(10):  \n",
    "        output = model(pos_input)\n",
    "        output = torch.squeeze(output)\n",
    "        output = output[-1]\n",
    "        div_tensor = torch.tensor([90,  180, 10000, 1000, 360, 1000]).to(device)\n",
    "        output = torch.mul(output, div_tensor)\n",
    "        output = output.view(1, 1, 6)\n",
    "        pos_input = torch.cat((pos_input, output), 1)\n",
    "\n",
    "print(pos_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
