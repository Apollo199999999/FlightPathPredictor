{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847e8d5a",
   "metadata": {},
   "source": [
    "# Train a Transformer model to predict the flight path of a plane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c77d11",
   "metadata": {},
   "source": [
    "This notebook assumes that you have ran the previous notebook to obtain training data, `retrieve_flight_training_data.ipynb`, and that PyTorch (and other standard data science related libraries) are already installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9400cf6",
   "metadata": {},
   "source": [
    "## 1. Imports and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba9f4c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Context size of our custom model\n",
    "context_size = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca72b99",
   "metadata": {},
   "source": [
    "## 2. Create a custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8768955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlightDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We are going to feed in positional data in batches of 1024 positions (we add 1 because when we get data we minus off last/first data point)\n",
    "        self.max_len = 1024 + 1\n",
    "\n",
    "        # We will not load all 80k csv files at once, we will load them on demand during training\n",
    "        # However, we will need to keep a record of which files correspond to which data points, so we can get them by index\n",
    "        self.train_files = []\n",
    "        self.train_sections = []\n",
    "        \n",
    "        for file in os.listdir(\"./data/csv\"):\n",
    "            if file.endswith(\".csv\") == False:\n",
    "                continue\n",
    "            # Read csv file and keep a record of it in the training lists\n",
    "            csv_path = os.path.join(\"./data/csv\", file)\n",
    "            with open(csv_path) as fp:\n",
    "                count = 0\n",
    "                for _ in fp:\n",
    "                    count += 1\n",
    "                # Exclude header row\n",
    "                count -= 1\n",
    "                num_sections = math.floor(count / self.max_len) + 1\n",
    "                for i in range(num_sections):\n",
    "                    self.train_files.append(csv_path)\n",
    "                    self.train_sections.append(i)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read corresponding csv file\n",
    "        csv_reader = pd.read_csv(self.train_files[idx])\n",
    "\n",
    "        # Convert csv data into a tensor\n",
    "        pos_embedding = torch.tensor(csv_reader.values, dtype=torch.float)\n",
    "        pos_embedding = pos_embedding.view(1, -1, 6)\n",
    "\n",
    "         # Split tensor based on max_len\n",
    "        num_sections = math.floor(pos_embedding.size()[1] / self.max_len) + 1\n",
    "        embedding_list = torch.tensor_split(pos_embedding, num_sections, dim=1)\n",
    "        \n",
    "        # Get the tensor to return\n",
    "        pos_sentence = embedding_list[self.train_sections[idx]]\n",
    "        \n",
    "        # Pad if necessary\n",
    "        tensor_length = pos_sentence.size()[1]\n",
    "        if tensor_length < self.max_len:\n",
    "            num_to_pad = self.max_len - tensor_length\n",
    "            zero_tensor = torch.zeros([1, num_to_pad, 6])\n",
    "            pos_sentence = torch.cat((pos_sentence, zero_tensor), 1)\n",
    "\n",
    "        pos_sentence = torch.squeeze(pos_sentence)\n",
    "        \n",
    "        return pos_sentence[:-1], pos_sentence[1:]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50183051-0f22-4db5-8acb-92070bf7e402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  55.5069,   -4.7423, 2875.0000,  247.9000,  218.1000, 2208.0000],\n",
      "        [  55.5050,   -4.7448, 2950.0000,  248.3000,  218.6000, 2496.0000],\n",
      "        [  55.5033,   -4.7473, 3050.0000,  248.8000,  219.1000, 2496.0000],\n",
      "        ...,\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000]])\n",
      "tensor([[  55.5050,   -4.7448, 2950.0000,  248.3000,  218.6000, 2496.0000],\n",
      "        [  55.5033,   -4.7473, 3050.0000,  248.8000,  219.1000, 2496.0000],\n",
      "        [  55.5016,   -4.7498, 3150.0000,  249.3000,  219.6000, 2496.0000],\n",
      "        ...,\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Test our custom dataset\n",
    "dataset = FlightDataset()\n",
    "sample, target = dataset.__getitem__(3)\n",
    "print(sample)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c1b5d-1eae-4706-b4a7-d2e3bc1a7525",
   "metadata": {},
   "source": [
    "## 3. Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf959203-2743-4f5a-a89d-9e194fdd6f5c",
   "metadata": {},
   "source": [
    "### Position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "768549b0-3da6-4a96-aeeb-d2fc1a4a20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(torch.nn.Module):\n",
    "    \"\"\"Token and positioning embedding layer for a sequence.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Init variables and layers.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.position_emb = torch.nn.Embedding(num_embeddings=context_size, embedding_dim=6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass.\"\"\"\n",
    "        len_input = x.size()[1]\n",
    "        positions = torch.arange(start=0, end=len_input, step=1).to(device)\n",
    "        position_embedding = self.position_emb(positions)\n",
    "        return x + position_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64c265-5d3e-4465-a768-d5024a3707bc",
   "metadata": {},
   "source": [
    "### Attention mask function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "348599e8-50ef-46f8-ae36-c7da24391a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask(key_length, query_length, dtype):\n",
    "    \"\"\"\n",
    "    Create a Casual Mask for\n",
    "    the multi head attention layer.\n",
    "    \"\"\"\n",
    "    i = torch.arange(query_length)[:, None]\n",
    "    j = torch.arange(key_length)\n",
    "    mask = i >= j - key_length + query_length\n",
    "    mask = torch.logical_not(mask)\n",
    "    mask = mask.to(dtype)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c3b51-72fc-4a61-859a-4574d55ceb7e",
   "metadata": {},
   "source": [
    "### Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d6bbb56-851a-42c4-b46a-28d32c746b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    \"\"\"Transformer Block Layer.\"\"\"\n",
    "    def __init__(self, num_heads, embed_dim, ff_dim, mask_function, dropout_rate=0.1):\n",
    "        \"\"\"Init variables and layers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.MultiheadAttention(\n",
    "          embed_dim=embed_dim,\n",
    "          num_heads=num_heads,\n",
    "          batch_first=True,\n",
    "        )\n",
    "        self.dropout_1 = torch.nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(\n",
    "          normalized_shape=embed_dim, eps=1e-6\n",
    "        )\n",
    "        self.ffn_1 = torch.nn.Linear(\n",
    "          in_features=embed_dim, out_features=ff_dim\n",
    "        )\n",
    "        self.ffn_2 = torch.nn.Linear(\n",
    "          in_features=ff_dim, out_features=embed_dim\n",
    "        )\n",
    "        self.dropout_2 = torch.nn.Dropout(p=dropout_rate)\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm(\n",
    "          normalized_shape=embed_dim, eps=1e-6\n",
    "        )\n",
    "        self.mask_function = mask_function\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward Pass.\"\"\"\n",
    "        seq_len = inputs.size()[1]\n",
    "        mask = self.mask_function(seq_len, seq_len, torch.bool).to(device)\n",
    "        attention_output, _ = self.attn(\n",
    "        query=inputs, key=inputs, value=inputs, attn_mask=mask\n",
    "        )\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        out1 = self.layer_norm_1(inputs + attention_output)\n",
    "        ffn_1 = self.relu(self.ffn_1(out1))\n",
    "        ffn_2 = self.ffn_2(ffn_1)\n",
    "        ffn_output = self.dropout_2(ffn_2)\n",
    "        output = self.layer_norm_2(out1 + ffn_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c77c53-c73d-414d-914f-4b9a45cae4fd",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "781d62d0-6328-4348-83cf-07d96c8a8d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlightModel(torch.nn.Module):\n",
    "  def __init__(self, feed_forward_dim, num_heads):\n",
    "    \"\"\"Init Function.\"\"\"\n",
    "    super().__init__()\n",
    "    self.embedding_layer = PositionEmbedding()\n",
    "    self.transformer_layers = []\n",
    "    for i in range(24):\n",
    "        transformer = TransformerBlock(\n",
    "          num_heads=num_heads,\n",
    "          embed_dim=6,\n",
    "          ff_dim=feed_forward_dim,\n",
    "          mask_function=create_attention_mask,\n",
    "        ).to(device)\n",
    "        self.transformer_layers.append(transformer)\n",
    "        \n",
    "    self.output_layer = torch.nn.Linear(6, 6)\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    \"\"\"Forward Pass.\"\"\"\n",
    "    # Position embedding\n",
    "    embedding = self.embedding_layer(input_tensor)\n",
    "    # Transformer layers\n",
    "    transformer_output = self.transformer_layers[0](embedding)\n",
    "    for i in range(1, len(self.transformer_layers)):\n",
    "        transformer_output = self.transformer_layers[i](transformer_output)\n",
    "    # FC network\n",
    "    output = self.output_layer(transformer_output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972110f5-1ffb-45e0-a20f-c1d1c3d610fb",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac420ef2-b00d-4cc6-956c-33c5fef6407b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Training Started.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.50 GiB is allocated by PyTorch, and 102.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_train_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m         train_losses\u001b[38;5;241m.\u001b[39mappend(running_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[44], line 20\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(model, num_epochs)\u001b[0m\n\u001b[0;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n\u001b[0;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[42], line 25\u001b[0m, in \u001b[0;36mFlightModel.forward\u001b[1;34m(self, input_tensor)\u001b[0m\n\u001b[0;32m     23\u001b[0m transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_layers[\u001b[38;5;241m0\u001b[39m](embedding)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_layers)):\n\u001b[1;32m---> 25\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# FC network\u001b[39;00m\n\u001b[0;32m     27\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(transformer_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[29], line 32\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     30\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     31\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_function(seq_len, seq_len, torch\u001b[38;5;241m.\u001b[39mbool)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 32\u001b[0m attention_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_1(attention_output)\n\u001b[0;32m     36\u001b[0m out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_1(inputs \u001b[38;5;241m+\u001b[39m attention_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1343\u001b[0m         query,\n\u001b[0;32m   1344\u001b[0m         key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:6242\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   6240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6241\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m-> 6242\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m   6244\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m dropout(attn_output_weights, p\u001b[38;5;241m=\u001b[39mdropout_p)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2140\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   2138\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2140\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.50 GiB is allocated by PyTorch, and 102.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model = FlightModel(24, 6).to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=filter(lambda param: param.requires_grad, model.parameters()), lr=0.01)\n",
    "print(\"Loading Data...\")\n",
    "train_dataset = FlightDataset()\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "train_losses = []\n",
    "\n",
    "def train_network(model, num_epochs):\n",
    "    \"\"\"Train the Network.\"\"\"\n",
    "    print(\"Training Started.\")\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        running_train_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {running_train_loss / len(train_loader)}\")\n",
    "        train_losses.append(running_train_loss / len(train_loader))\n",
    "\n",
    "train_network(model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82a90582-d830-4a47-a451-bdc9fa88ec94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a163d-a6f6-4a57-a9e3-1a80dfd0d9eb",
   "metadata": {},
   "source": [
    "## 5. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d965549-628b-4a71-9968-c63362528f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./flight_prediction_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
